0) Introduction
As it is well known, the IP protocol used to transport data through the Internet shreds the binary file into small tiles called datagrams, which are sent one by one separately.
As it it also known, the Internet connection is not perfect. Bit failures occur, sometimes whole datagrams disappear. To address the first problem, mistake finding and 
correction algorithms exist. The purpose of this project is to compare existing and possible mistake finding approaches. 

The MTU(Maximum Translation Unit) defines maximum datagram size. Most of the packages sent will be exactly this size(with no regard to IP-overhead for this project). 
For Ethernet II this amounts to 1500 bytes (RFC 1191). 
Other important values used:
Minimal Translation Unit(Ethernet)- 64 bytes - due to specialities of Internet, it is often used

There are four possible outcomes of a datagram transfer: 
it was transported safely and accepted(later referred as True Positive); 
it was corrupted but that was noticed (True Negative);
the initial data was not corrupted, but the correction data was, so it was rejected(False Negative)
mistakes occured both in the data and the correction algorithm, so it was accepted despite being corrupt(False Positive)

These are listed in order of rising danger. True Negatives and False Negatives just demand a datagram re-sent(the second ones without a proper reason). 
One False Positive can corrupt a whole file. This may not be important, when the object is an image, video or a sound recording, but a text or worse, a binary code
will be corrupted irreparably. Therefore the main job of a mistake finder is to address False Positives. All the algorithms will be tweaked to prevent this kind of result 
as much as possible, even if it means neglecting possibilities to recognise many more False Negatives.

File lengths to compare:
"War and peace" by Leo Tolstoy- 747.323 characters aka 747.323 byte aka 1 MB(with spaces, formatting headers, etc)
"Doom",1993 by Id Software - 40 MB, aka 40.000.000 byte
Average music track, 5 munites long, mp3 format - 4 MB, aka 4.000.000 byte
Average photo filmed by an average phone, jpg format - 100 KB, aka 100.000 byte

1) Singular Parity Bits(SPB)
The simplest approach possible.
 In this example, ASCII symbols will be used. Each of them takes exactly 7 bits, so in every byte there is a spare bit that carries no information.
 The 1st bit in each byte is now a parity bit; it is set so, that the sum of bits with it is even. If the checking process finds out, that a byte has 
 an uneven sum, it will be framed as corrupt.

 Math behind the method: 
To create a False Positive, an even number of bits in a row must be corrupted. 
Each of 8 bits can be corrupted and has 7 other bits in the same byte, with corruption chance 1 500 000 for each.
That gives approximately a corruption chance of (7*8/(CORRUPT_CHANCE^2)+ 7*8/(CORRUPT_CHANCE^4) + 7*8/(CORRUPT_CHANCE^6) +7*8/(CORRUPT_CHANCE^8))* datagramBytes
													2 bits					4 bits						6 bits					8 bits
As everything after 2 bits is hugely improbable (1.500.000^2 less possible), they will not be accounted for neither now nor later.
So, that's approximately 3.7 *10^-8 (1 error in ~27 MB)

 Result:
 Check 1 (1500 byte datagram):
Total bytes    1 500 000 000
Total True Positive    693298           Persentage: 69.3298
Total True Negative    274229           Persentage: 27.4229
Total False Positive    21              Persentage: 0.0021
Total False Negative    32452           Persentage: 3.2452

Check 2 (1500 byte datagram):
Total bytes    1 500 000 000
Total True Positive    693543           Persentage: 69.3543
Total True Negative    273833           Persentage: 27.3833
Total False Positive    15              Persentage: 0.0015
Total False Negative    32609           Persentage: 3.2609

Check 3 (64 byte datagram):
Total bytes    64 000 000
Total True Positive    984510           Persentage: 98.451
Total True Negative    13564            Persentage: 1.3564
Total False Positive    1               Persentage: 0.0001
Total False Negative    1925            Persentage: 0.1925

Check 4 (64 byte datagram):
Total bytes    64 000 000
Total True Positive    984521           Persentage: 98.4521
Total True Negative    13527            Persentage: 1.3527
Total False Positive    1               Persentage: 0.0001
Total False Negative    1951            Persentage: 0.1951

Check 5(300 byte datagram):
Total bytes    300 000 000
Total True Positive    929295           Persentage: 92.9295
Total True Negative    61986            Persentage: 6.1986
Total False Positive    4               Persentage: 0.0004
Total False Negative    8715            Persentage: 0.8715

Check 6(750 byte datagram):
Total bytes    750 000 000
Total True Positive    832362           Persentage: 83.2362
Total True Negative    148330           Persentage: 14.833
Total False Positive    16              Persentage: 0.0016
Total False Negative    19292           Persentage: 1.9292

To sum up, there's about 1 error in 30 000 000 byte aka 30MB.
It can be seen that SPB cannot be relied upon, when it comes to transferring files. Actually, it seems to perform worse on smaller datagram- I assume, more mistakes happen in bigger
datagrams, so there are more possibilities to catch them. False Negatives also occur often, but it is unavoidable as no error correction happens.
Additionally, SPB takes a lot of space: although no extra space is needed in our case, when only ASCII symbols are transferred, otherwise it would expand the file by extra 1/8.
The optimal datagram size would be ~300 byte, so that True Positive rate is kept above 90%.
But let's give it another chance. What if there was another layer of security?

2) Double Parity Check
Parity Check is back, in its improved version.
Twin parity bit is an variant of parity bit, which also has each 8th byte as parity bit byte. Diagram below:
a x x x x x x x 			x- regular bit
a x x x x x x x 			a- parity bit, like in simple approach
a x x x x x x x 			b- additional row of parity bits, each checking its column
a x x x x x x x 			c- has to check column a and row b simutaneously, that is impossible. Therefore checks only column a
a x x x x x x x
a x x x x x x x
a x x x x x x x
c b b b b b b b

This variant is very space-consuming even with the simplification taken, each 8th bit was extra added. In a full realisation, 15 of 64 bits are parity bits- that's even more.
1312 bit datagrams are being used here to address this problem. The resulting(protected) datagrams will be exactly 1500 bit long.

Math behind the method:
To corrupt a bit, the following bits must be corrupted:
1) one regular bit	(there are 64 in each byteblock)						
2) any other bit in this row	(7 of them)
3) any bit in the first column		(also 7)
4) the bit in the second column and third row	(only one)
Any other variants will make at least 1 more step, being 1.500.000 times less possible. So, they will not be calculated.
Final formula is: 64*7*7/(CORRUPT_CHANCE^4)*(datagramBytes/7).					(each byteblock is 8 byte, 7 of them are initial data)
Result: 1 error per 10^18 bit. 
In reality it must be even smaller, as any True Negative or False Negative will abort the whole datagram. (The chance for it being appr. (1/CORRUPT_CHANCE)* (datagramBytes*8) )

Result:
Check 1(1312 byte datagram):
Total bytes    1 500 000 000
Total True Positive    693389           Persentage: 69.3389
Total True Negative    244225           Persentage: 24.4225
Total False Positive    0               Persentage: 0
Total False Negative    62386           Persentage: 6.2386

Check 2(1500 byte):
Total bytes    1 715 000 000
Total True Positive    658064           Persentage: 65.8064
Total True Negative    274341           Persentage: 27.4341
Total False Positive    0               Persentage: 0
Total False Negative    67595           Persentage: 6.7595

Check 3(1312 byte):
Total bytes    1 500 000 000
Total True Positive    693321           Persentage: 69.3321
Total True Negative    244517           Persentage: 24.4517
Total False Positive    0               Persentage: 0
Total False Negative    62162           Persentage: 6.2162

This is where problems arise. This last one has took hours to complete and yet it has found no False Positives. I'll have to increase error probability to be able to analyse further.
Remember the formula 64*7*7/(CORRUPT_CHANCE^4)*(datagramBytes/7)? According to it, an decrease of CORRUPT_CHANCE increases the error probability by 10^4=10 000.

Check 4(1312 byte, 100x bit error probability)
Total bytes    1 500 000 000
Total True Positive    334459           Persentage: 33.4459
Total True Negative    567563           Persentage: 56.7563
Total False Positive    0               Persentage: 0
Total False Negative    97978           Persentage: 9.7978

Check 5(1312 byte, 1000x errors)
Total bytes    1 500 000 000
Total True Positive    314              Persentage: 0.0314
Total True Negative    997829           Persentage: 99.7829
Total False Positive    0               Persentage: 0
Total False Negative    1857            Persentage: 0.1857

The further tests can be even more untrue due to their small size.
Check 6(15 byte, 100.000x errors)
Total bytes    18 000 000
Total True Positive    43               Persentage: 0.0043
Total True Negative    999314           Persentage: 99.9314
Total False Positive    2               Persentage: 0.0002
Total False Negative    641             Persentage: 0.0641

Check 7(15 byte, 100.000x errors)
Total bytes    180 000 000
Total True Positive    460              Persentage: 0.0046
Total True Negative    9992982          Persentage: 99.9298
Total False Positive    26              Persentage: 0.00026
Total False Negative    6532            Persentage: 0.06532


To sum up: this one is quite safe, unlike its predecessor. Only 15-byte sequences with extremely high corruption chance have False Positives.
A big size increase, howewer, makes matters worse; but with current Internet speeds it can be, perhaps, allowed.

3)Checksums
This is the first of the methods that is actually used. Both UDP and TCP use checksums. Although they are a level above IP that we're researching now, checksums can still 
be used for our purpose.
To calculate a checksum, the file is "cut" in 16-bit parts, which are summed together one-by-one. If a leading 17th bit emerges, it is moved to the end and added to the 1st bit.
At last, the checksum is inverted, then stored in the end of the datagram.
To check correctness, the same process is done once again(without inverting), the two checksums are summed up, and the result should be 16 1-bits. Or they are just compared.

Math behind the method:
To cause a False Positive, you have to corrupt two bits with such numbers num1 and num2, so that num1%16=num2%16.
Therefore, the error chance is (datagramBytes/16+1)*(datagramBytes/16)/2*16/CORRUPT_CHANCE^2. This is heavily dependant the message size, so bigger messages will be more prone to 
errors. It's therefore wise to only encode smaller texts with this method.
Check 1(1500 byte):
Total bytes    150 200 000
Total True Positive    69244            Persentage: 69.244
Total True Negative    30580            Persentage: 30.58
Total False Positive    117             Persentage: 0.117
Total False Negative    59              Persentage: 0.059

Check 2(1500 byte):
Total bytes    150 200 000
Total True Positive    69326            Persentage: 69.326
Total True Negative    30504            Persentage: 30.504
Total False Positive    109             Persentage: 0.109
Total False Negative    61              Persentage: 0.061

Check 3(1498 byte):
Total bytes    150000000
Total True Positive    69214            Persentage: 69.214
Total True Negative    30608            Persentage: 30.608
Total False Positive    116             Persentage: 0.116
Total False Negative    62              Persentage: 0.062

Check 4(1498 byte):
Total bytes    1 500 000 000
Total True Positive    693341           Persentage: 69.3341
Total True Negative    304783           Persentage: 30.4783
Total False Positive    1184            Persentage: 0.1184
Total False Negative    692             Persentage: 0.0692

Check 5(98 byte)
Total bytes    100000000
Total True Positive    975998           Persentage: 97.5998
Total True Negative    23057            Persentage: 2.3057
Total False Positive    6               Persentage: 0.0006
Total False Negative    939             Persentage: 0.0939

Check 5(498 byte)
Total bytes    500000000
Total True Positive    884966           Persentage: 88.4966
Total True Negative    113998           Persentage: 11.3998
Total False Positive    160             Persentage: 0.016
Total False Negative    876             Persentage: 0.0876

This is clearly bad, much worse then Parity Bits. Even in smaller messages where it's supposed to shine, the checksum fails to avoid False Positives.
It seems unreasonable to try anymore, so other checksum variants won't be touched here.

4) Double sending
What if the same package should be sent several times? I'll start with two.

Math behind the method:
To cause a False Positive, you have to corrupt the same bit in both variants of the message. The chance of this occurance is 1/CORRUPTCHANCE^2.
Check 1(750 byte):
Total bytes    300 000 000
Total True Positive    138770           Persentage: 69.385
Total True Negative    33351            Persentage: 16.6755
Total False Positive    1               Persentage: 0.0005
Total False Negative    27878           Persentage: 13.939

Check 2(750 byte):
Total bytes    150 000 000
Total True Positive    69370            Persentage: 69.37
Total True Negative    16762            Persentage: 16.762
Total False Positive    0               Persentage: 0
Total False Negative    13868           Persentage: 13.868

Check 3(750 byte):
Total bytes    1 500 000 000
Total True Positive    693589           Persentage: 69.3589
Total True Negative    167294           Persentage: 16.7294
Total False Positive    2               Persentage: 0.0002
Total False Negative    139115          Persentage: 13.9115

Check 4(150 byte):
Total bytes    600 000 000
Total True Positive    1858548          Persentage: 92.9274
Total True Negative    72248            Persentage: 3.6124
Total False Positive    0               Persentage: 0
Total False Negative    69204           Persentage: 3.4602

Check 5(750 byte):
Total bytes    600 000 000
Total True Positive    1858948          Persentage: 92.9474
Total True Negative    71876            Persentage: 3.5938
Total False Positive    6               Persentage: 0.0003
Total False Negative    69170           Persentage: 3.4585

The results are, expectedly, not good enough. And the workload is immense: literally twice the data. 
There's nothing else to say here. 

5) Triple sending
Perhaps there can be another chance for the paradigm. After all, the technologies like 10G Ethernet allow an astonishing bandwidth. 
Now all the data is being sent thrice. 

Math behind the method:
To cause a False Positive, you have to corrupt the same bit in both variants of the message. The chance of this occurance is 1/datagramBytes/CORRUPTCHANCE^3.

Check 1(500 byte):
Total bytes    150 000 000
Total True Positive    69484            Persentage: 69.484
Total True Negative    11441            Persentage: 11.441
Total False Positive    0               Persentage: 0
Total False Negative    19075           Persentage: 19.075

Check 2(500 byte):
Total bytes    1 500 000 000
Total True Positive    693462           Persentage: 69.3462
Total True Negative    115103           Persentage: 11.5103
Total False Positive    0               Persentage: 0
Total False Negative    191435          Persentage: 19.1435

Check 3(500 byte):
Total bytes    15 000 000 000
Total True Positive    6932105          Persentage: 69.321
Total True Negative    1150265          Persentage: 11.5027
Total False Positive    0               Persentage: 0
Total False Negative    1917630         Persentage: 19.1763

Check 4(50 byte, 10 000x errors):
Total bytes    150 000 000
Total True Positive    319              Persentage: 0.0319
Total True Negative    931151           Persentage: 93.1151
Total False Positive    0               Persentage: 0
Total False Negative    68530           Persentage: 6.853

Check 5(5 byte, 100 000x errors):
Total bytes    150 000 000
Total True Positive    2431             Persentage: 0.02431
Total True Negative    9368018          Persentage: 93.6802
Total False Positive    28              Persentage: 0.00028
Total False Negative    629523          Persentage: 6.29523

Check 6:(5 byte, 100 000x errors):
Total bytes    150 000 000
Total True Positive    2481             Persentage: 0.02481
Total True Negative    9367418          Persentage: 93.6742
Total False Positive    26              Persentage: 0.00026
Total False Negative    630075          Persentage: 6.30075

The overall result is really satisfying: 5 000 000 000 000 byte of data give us 25-30 False Positives. That amounts to approximately 1 False Positive per 185 GB.
But there's a cost. When reviewing Double Parity bit I said the extra workload is huge. Here, it is tremendous. Yes, this approach is easily scalable, as there is no difficulty in
sending one more copy, and there can even be a kind of error correction; but each new copy will increase the workload even greater. Thus, it would be wise to look for other approaches.
For now, at least. Maybe, with development of extremely high-velocity Internet, there will be a time to shine for this one.

6) CRC-32
At long last, there it is. CRC-32 is corrently being used in IP. I will be using the version described by IEEE 802.3.
The working principle is quite complex. To sum up, a polynomial is picked, called the generator polynomial(GP), with maximal power of 32. 
Both the sender and receiver know it beforehand. Then, the data is divided %2 by the GP. The rest from this division equals the CRC checksum. 
Generator polynomial: x^32+x^26+x^23+x^22+^16+x^12+x^11+x^10+x^8+x^7+x^5+x^4+x^2+x+1
In binary, it looks like this: 00000001.00000100.11000001.00011101.10110111

Math behind the method:
https://www.ieee802.org/3/bv/public/Jan_2015/perezaranda_3bv_2_0115.pdf
Shortly, the error probability is circa 1/2^32

Check 1(15 bit, 100 000x errors):
Total bytes    19 000 000
Total True Positive    18               Persentage: 0.0018
Total True Negative    999737           Persentage: 99.9737
Total False Positive    0               Persentage: 0
Total False Negative    245             Persentage: 0.0245

Check 2(15 bit, 100 000x errors):
Total bytes    19 000 000
Total True Positive    28               Persentage: 0.0028
Total True Negative    999726           Persentage: 99.9726
Total False Positive    0               Persentage: 0
Total False Negative    246             Persentage: 0.0246

Check 3(15 bit, 100 000x errors):
Total bytes    190 000 000
Total True Positive    281              Persentage: 0.00281
Total True Negative    9997476          Persentage: 99.9748
Total False Positive    0               Persentage: 0
Total False Negative    2243            Persentage: 0.02243

Check 4(1500 bit, regular errors):
Total bytes    1 504 000 000
Total True Positive    692690           Persentage: 69.269
Total True Negative    306641           Persentage: 30.6641
Total False Positive    0               Persentage: 0
Total False Negative    669             Persentage: 0.0669

Check 5(15 bit, 100 000x errors):
Total bytes    190 000 000
Total True Positive    275              Persentage: 0.00275
Total True Negative    9997498          Persentage: 99.975
Total False Positive    0               Persentage: 0
Total False Negative    2227            Persentage: 0.02227

This is the best one. Granting great protection from False Positives, CRC32 is the algorithm applied in most current use cases.

7)Technical commentaries:
-these address technical facts and difficulties. Mostly they have nothing to do with purpose of this study. Howewer, they can help if you want to simulate something, or
answer technical questions.
-please inform me if you possess any information about the bit corruption possibility. I assumed 1 bit in 1.500.000 gets corrupted. You can tweak the variable CORRUPT_CHANCE to change that.
-running 100.000 datagrams of maximal size already takes more than a minute. Don't run more unless you have enough time.
-debug mode is far slower then release mode. Use the latter only.